{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Sequential\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters in Here\n",
    "image_size = 224\n",
    "patch_size = 32\n",
    "n_patches = image_size // patch_size \n",
    "assert (image_size % patch_size == 0), 'image_size must be divisible by patch_size'\n",
    "projection_dim = 8\n",
    "transformer_layers = 2\n",
    "num_heads = 8\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "] \n",
    "num_classes = 2\n",
    "mlp_head_units = [2048, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_image = Image.open('../data/image.jpeg')\n",
    "np_image = np_image.resize((image_size, image_size), Image.Resampling.BILINEAR)\n",
    "np_image = np.asarray(np_image)\n",
    "np_image_batch = []\n",
    "i = 0\n",
    "while i < 10:\n",
    "    np_image_batch.append(np_image)\n",
    "    i += 1\n",
    "\n",
    "np_image_batch = np.asarray(np_image_batch)\n",
    "tensor_image_batch = tf.convert_to_tensor(np_image_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=(image_size, image_size, 3))\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(n_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)          [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " data_augmentation (Sequential)  (None, 224, 224, 3)  7          ['input_15[0][0]']               \n",
      "                                                                                                  \n",
      " patches_19 (Patches)           (None, None, 3072)   0           ['data_augmentation[13][0]']     \n",
      "                                                                                                  \n",
      " patch_encoder_14 (PatchEncoder  (None, 7, 8)        24640       ['patches_19[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " layer_normalization_65 (LayerN  (None, 7, 8)        16          ['patch_encoder_14[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_26 (Multi  (None, 7, 8)        2248        ['layer_normalization_65[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_65[0][0]'] \n",
      "                                                                                                  \n",
      " add_52 (Add)                   (None, 7, 8)         0           ['multi_head_attention_26[0][0]',\n",
      "                                                                  'patch_encoder_14[0][0]']       \n",
      "                                                                                                  \n",
      " layer_normalization_66 (LayerN  (None, 7, 8)        16          ['add_52[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_103 (Dense)              (None, 7, 16)        144         ['layer_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_91 (Dropout)           (None, 7, 16)        0           ['dense_103[0][0]']              \n",
      "                                                                                                  \n",
      " dense_104 (Dense)              (None, 7, 8)         136         ['dropout_91[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_92 (Dropout)           (None, 7, 8)         0           ['dense_104[0][0]']              \n",
      "                                                                                                  \n",
      " add_53 (Add)                   (None, 7, 8)         0           ['dropout_92[0][0]',             \n",
      "                                                                  'add_52[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_67 (LayerN  (None, 7, 8)        16          ['add_53[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_27 (Multi  (None, 7, 8)        2248        ['layer_normalization_67[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      " add_54 (Add)                   (None, 7, 8)         0           ['multi_head_attention_27[0][0]',\n",
      "                                                                  'add_53[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_68 (LayerN  (None, 7, 8)        16          ['add_54[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_105 (Dense)              (None, 7, 16)        144         ['layer_normalization_68[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_93 (Dropout)           (None, 7, 16)        0           ['dense_105[0][0]']              \n",
      "                                                                                                  \n",
      " dense_106 (Dense)              (None, 7, 8)         136         ['dropout_93[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_94 (Dropout)           (None, 7, 8)         0           ['dense_106[0][0]']              \n",
      "                                                                                                  \n",
      " add_55 (Add)                   (None, 7, 8)         0           ['dropout_94[0][0]',             \n",
      "                                                                  'add_54[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_69 (LayerN  (None, 7, 8)        16          ['add_55[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " flatten_13 (Flatten)           (None, 56)           0           ['layer_normalization_69[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_95 (Dropout)           (None, 56)           0           ['flatten_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_107 (Dense)              (None, 2048)         116736      ['dropout_95[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_96 (Dropout)           (None, 2048)         0           ['dense_107[0][0]']              \n",
      "                                                                                                  \n",
      " dense_108 (Dense)              (None, 1024)         2098176     ['dropout_96[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_97 (Dropout)           (None, 1024)         0           ['dense_108[0][0]']              \n",
      "                                                                                                  \n",
      " dense_109 (Dense)              (None, 2)            2050        ['dropout_97[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,246,745\n",
      "Trainable params: 2,246,738\n",
      "Non-trainable params: 7\n",
      "__________________________________________________________________________________________________\n",
      "<keras.engine.input_layer.InputLayer object at 0x7fcb4a482950>\n",
      "Model: \"data_augmentation\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  (None, 224, 224, 3)      7         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " resizing (Resizing)         (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " random_flip (RandomFlip)    (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " random_rotation (RandomRota  (None, 224, 224, 3)      0         \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " random_zoom (RandomZoom)    (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7\n",
      "Trainable params: 0\n",
      "Non-trainable params: 7\n",
      "_________________________________________________________________\n",
      "None\n",
      "<__main__.Patches object at 0x7fcb4a1de020>\n",
      "<__main__.PatchEncoder object at 0x7fcb4a430880>\n",
      "<keras.layers.normalization.layer_normalization.LayerNormalization object at 0x7fcb4a449360>\n",
      "<keras.layers.attention.multi_head_attention.MultiHeadAttention object at 0x7fcb4a1dd960>\n",
      "<keras.layers.merging.add.Add object at 0x7fcb4a019510>\n",
      "<keras.layers.normalization.layer_normalization.LayerNormalization object at 0x7fcb4a01b2b0>\n",
      "<keras.layers.core.dense.Dense object at 0x7fcb49fa85b0>\n",
      "<keras.layers.regularization.dropout.Dropout object at 0x7fcb49faad70>\n",
      "<keras.layers.core.dense.Dense object at 0x7fcb49fab040>\n",
      "<keras.layers.regularization.dropout.Dropout object at 0x7fcb49fa9690>\n",
      "<keras.layers.merging.add.Add object at 0x7fcb4797af80>\n",
      "<keras.layers.normalization.layer_normalization.LayerNormalization object at 0x7fcb479fafe0>\n",
      "<keras.layers.attention.multi_head_attention.MultiHeadAttention object at 0x7fcb49f66a10>\n",
      "<keras.layers.merging.add.Add object at 0x7fcb4992d900>\n",
      "<keras.layers.normalization.layer_normalization.LayerNormalization object at 0x7fcb4992cfd0>\n",
      "<keras.layers.core.dense.Dense object at 0x7fcb4992c610>\n",
      "<keras.layers.regularization.dropout.Dropout object at 0x7fcb479f8370>\n",
      "<keras.layers.core.dense.Dense object at 0x7fcb479fb5e0>\n",
      "<keras.layers.regularization.dropout.Dropout object at 0x7fcb49f66b90>\n",
      "<keras.layers.merging.add.Add object at 0x7fcb4992e170>\n",
      "<keras.layers.normalization.layer_normalization.LayerNormalization object at 0x7fcb479fa9e0>\n",
      "<keras.layers.reshaping.flatten.Flatten object at 0x7fcb499e8f40>\n",
      "<keras.layers.regularization.dropout.Dropout object at 0x7fcb47978670>\n",
      "<keras.layers.core.dense.Dense object at 0x7fcb49f67cd0>\n",
      "<keras.layers.regularization.dropout.Dropout object at 0x7fcb46c3f4f0>\n",
      "<keras.layers.core.dense.Dense object at 0x7fcb49fefb80>\n",
      "<keras.layers.regularization.dropout.Dropout object at 0x7fcb4797a4a0>\n",
      "<keras.layers.core.dense.Dense object at 0x7fcb4797b4c0>\n"
     ]
    }
   ],
   "source": [
    "model = create_vit_classifier()\n",
    "model.summary()\n",
    "for layer in model.layers:\n",
    "    try:\n",
    "        print(layer.summary())\n",
    "    except:\n",
    "        print(layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('MedicalClassificationML-tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a18bcef3509e3fe6906f1751551d33c3fd853060613b86c696e737e046fa81c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
